{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI26F4AfR7lv",
        "outputId": "bdac2ae2-052f-4076-91b8-e06800874213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.1\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "import keras\n",
        "print( keras.__version__ )#version 2.1.2\n",
        "from keras import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fn = '50EleReviews.json' #origial review documents, there are 50 classes\n",
        "with open(fn, 'r') as infile:\n",
        "        docs = json.load(infile)\n",
        "X = docs['X']\n",
        "y = np.asarray(docs['y'])\n",
        "num_classes = len(docs['target_names'])\n",
        "\n",
        "'''\n",
        "50EleReviews.json\n",
        "y : size =  50000\n",
        "X : size =  50000\n",
        "target_names : size =  50\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Xnvio1k2SK2Z",
        "outputId": "30701464-8e75-42ce-9437-ca91a4a05f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n50EleReviews.json\\ny : size =  50000\\nX : size =  50000\\ntarget_names : size =  50\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count each word's occurance\n",
        "def count_word(X):\n",
        "    word_count = dict()\n",
        "    for d in X:\n",
        "        for w in d.lower().split(' '): #lower\n",
        "            if w in word_count:\n",
        "                word_count[w] += 1\n",
        "            else:\n",
        "                word_count[w] = 1\n",
        "    return word_count\n",
        "\n",
        "word_count = count_word(X)\n",
        "print ('total words: ', len(word_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYIv9emKSQGQ",
        "outputId": "44016f11-3ce2-41f8-ef85-59c3c72c9c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words:  79259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get frequent words\n",
        "freq_words = [w  for w, c in word_count.items() if c > 10]\n",
        "print( 'frequent word size = ', len(freq_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zdVd0yyS4Uj",
        "outputId": "37a8fe9f-a008-4d0c-dcd5-a80f127d08d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frequent word size =  12241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx = {w: i+2  for i, w in enumerate(freq_words)} # index 0 for padding, index 1 for unknown/rare words\n",
        "idx_to_word = {i:w for w, i in word_to_idx.items()}"
      ],
      "metadata": {
        "id": "0tzSixOzS7u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_word(X):\n",
        "    seqs = []\n",
        "    max_length = 0\n",
        "    for d in X:\n",
        "        seq = []\n",
        "        for w in d.lower().split():\n",
        "            if w in word_to_idx:\n",
        "                seq.append(word_to_idx[w])\n",
        "            else:\n",
        "                seq.append(1) #rare word index = 1\n",
        "        seqs.append(seq)\n",
        "    return seqs"
      ],
      "metadata": {
        "id": "YtQhL4AdTVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_X = index_word(X)\n",
        "padded_X = keras.utils.pad_sequences(indexed_X, maxlen=3000, dtype='int32', padding='post', truncating='post', value = 0.)"
      ],
      "metadata": {
        "id": "0F189gHbTaOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTest(X, y, ratio = 0.7): # 70% for training, 30% for testing\n",
        "    shuffle_idx = np.random.permutation(len(y))\n",
        "    split_idx = int(0.7*len(y))\n",
        "    shuffled_X = X[shuffle_idx]\n",
        "    shuffled_y = y[shuffle_idx]\n",
        "\n",
        "    return shuffled_X[:split_idx], shuffled_y[:split_idx], shuffled_X[split_idx:], shuffled_y[split_idx:]\n",
        "\n",
        "train_X, train_y, test_X, test_y = splitTrainTest(padded_X, y)\n",
        "\n",
        "print( train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVuLjTCmTgAR",
        "outputId": "284d89f1-35b2-4514-9d7d-922a080db558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35000, 3000) (35000,) (15000, 3000) (15000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape, train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB5OJIGVXnT0",
        "outputId": "9570ed9b-2d0d-4a0a-c343-d045dddc637a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 3000), (35000,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def splitSeenUnseen(X, y, seen, unseen):\n",
        "    seen_mask = np.in1d(y, seen)# find examples whose label is in seen classes\n",
        "    unseen_mask = np.in1d(y, unseen)# find examples whose label is in unseen classes\n",
        "\n",
        "    print( np.array_equal(np.logical_and(seen_mask, unseen_mask), np.zeros((y.shape), dtype= bool)))#expect to see 'True', check two masks are exclusive\n",
        "\n",
        "    # map elements in y to [0, ..., len(seen)] based on seen, map y to unseen_label when it belongs to unseen classes\n",
        "    to_seen = {l:i for i, l in enumerate(seen)}\n",
        "    unseen_label = len(seen)\n",
        "    to_unseen = {l:unseen_label for l in unseen}\n",
        "\n",
        "    return X[seen_mask], np.vectorize(to_seen.get)(y[seen_mask]), X[unseen_mask], np.vectorize(to_unseen.get)(y[unseen_mask])\n",
        "\n",
        "seen = range(24)#seen classes\n",
        "unseen = range(24,50)#unseen classes\n",
        "\n",
        "seen_train_X, seen_train_y, _, _ = splitSeenUnseen(train_X, train_y, seen, unseen)\n",
        "seen_test_X, seen_test_y, unseen_test_X, unseen_test_y = splitSeenUnseen(test_X, test_y, seen, unseen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbX9b_6vTkwL",
        "outputId": "cbaa58b6-9a6e-4706-f3a6-06c7598c445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seen_train_X.shape, seen_train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOvaPS5YiE5",
        "outputId": "f9460028-b105-418e-bdc1-fafdc74318f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16866, 3000), (16866,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes and the percentage to keep for the last 13 classes\n",
        "num_classes = 24\n",
        "percentage_to_keep = 0.20\n",
        "\n",
        "# Create a mask for the first 12 classes\n",
        "mask_first_12_classes = seen_train_y < 12\n",
        "\n",
        "# Create a mask for the last 13 classes\n",
        "mask_last_12_classes = seen_train_y >= 12\n",
        "\n",
        "# Count the number of data points for the last 13 classes\n",
        "num_data_points_last_12_classes = np.sum(mask_last_12_classes)\n",
        "\n",
        "# Randomly select 30% of the data points for the last 13 classes\n",
        "num_to_keep_last_12_classes = int(num_data_points_last_12_classes * percentage_to_keep)\n",
        "selected_indices = np.random.choice(np.where(mask_last_12_classes)[0], num_to_keep_last_12_classes, replace=False)\n",
        "\n",
        "# Combine the masks to get the final mask\n",
        "final_mask = np.logical_or(mask_first_12_classes, np.isin(np.arange(len(seen_train_y)), selected_indices))\n",
        "\n",
        "# Apply the final mask to filter train_X and train_y\n",
        "filtered_train_X = seen_train_X[final_mask]\n",
        "filtered_train_y = seen_train_y[final_mask]"
      ],
      "metadata": {
        "id": "Y910fpAiZ3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seen_train_X = filtered_train_X\n",
        "seen_train_y = filtered_train_y"
      ],
      "metadata": {
        "id": "svhyIylvapsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seen_train_X.shape, seen_train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6YV69BglCmk",
        "outputId": "a3d32710-efda-417e-af17-6c382770c37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10133, 3000), (10133,))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = np.bincount(seen_train_y)\n",
        "\n",
        "# Create a list of class numbers\n",
        "class_numbers = list(range(len(class_counts)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a bar graph to visualize the class distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(class_numbers, class_counts, tick_label=class_numbers)\n",
        "plt.xlabel('Class Number')\n",
        "plt.ylabel('Number of Data Points')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "IUIrlxW1ZoFY",
        "outputId": "2071bc18-6ec2-4221-cf3d-5f4b099329fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Assuming you have seen_train_y containing the training labels\n",
        "# Adjust the number of classes as needed\n",
        "num_classes = 24\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(seen_train_y), y=seen_train_y)\n",
        "\n",
        "# Create a dictionary of class weights\n",
        "class_weight_dict = {}\n",
        "for class_idx, class_weight in enumerate(class_weights):\n",
        "    class_weight_dict[class_idx] = class_weight\n",
        "\n",
        "class_weight_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR7utGTQlVLj",
        "outputId": "3b4c6d7f-6d0b-4c2c-8cba-112704812eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.5791609510745314,\n",
              " 1: 0.5997277462121212,\n",
              " 2: 0.5980288007554296,\n",
              " 3: 0.6083693563880884,\n",
              " 4: 0.5988770685579197,\n",
              " 5: 0.5831606813996317,\n",
              " 6: 0.5872160407974037,\n",
              " 7: 0.6329960019990005,\n",
              " 8: 0.5963394538606404,\n",
              " 9: 0.6118961352657005,\n",
              " 10: 0.6074940047961631,\n",
              " 11: 0.5954983544898919,\n",
              " 12: 3.324475065616798,\n",
              " 13: 2.741612554112554,\n",
              " 14: 2.852759009009009,\n",
              " 15: 2.9525058275058274,\n",
              " 16: 3.7363569321533925,\n",
              " 17: 3.1044730392156863,\n",
              " 18: 2.689225053078556,\n",
              " 19: 2.814722222222222,\n",
              " 20: 2.994385342789598,\n",
              " 21: 3.2985026041666665,\n",
              " 22: 2.994385342789598,\n",
              " 23: 2.9117816091954025}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have filtered_train_y containing the filtered labels\n",
        "# You can adjust the threshold as needed\n",
        "# HEAD TAIL SPLIT\n",
        "'''threshold = 100  # Adjust this threshold based on your dataset and requirements\n",
        "\n",
        "# Count the number of data points for each class\n",
        "class_counts = np.bincount(filtered_train_y)\n",
        "\n",
        "# Find the class indices that meet the threshold criteria\n",
        "tail_class_indices = np.where(class_counts < threshold)[0]\n",
        "head_class_indices = np.where(class_counts >= threshold)[0]\n",
        "\n",
        "# Create masks to separate the data into head and tail classes\n",
        "is_tail_class = np.isin(filtered_train_y, tail_class_indices)\n",
        "is_head_class = np.isin(filtered_train_y, head_class_indices)\n",
        "\n",
        "# Separate the data into head and tail classes\n",
        "tail_class_data_X = filtered_train_X[is_tail_class]\n",
        "tail_class_data_y = filtered_train_y[is_tail_class]\n",
        "\n",
        "head_class_data_X = filtered_train_X[is_head_class]\n",
        "head_class_data_y = filtered_train_y[is_head_class]'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "AjWs9anfbrUA",
        "outputId": "dec34d46-6275-44cb-9088-7c690445c6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'threshold = 100  # Adjust this threshold based on your dataset and requirements\\n\\n# Count the number of data points for each class\\nclass_counts = np.bincount(filtered_train_y)\\n\\n# Find the class indices that meet the threshold criteria\\ntail_class_indices = np.where(class_counts < threshold)[0]\\nhead_class_indices = np.where(class_counts >= threshold)[0]\\n\\n# Create masks to separate the data into head and tail classes\\nis_tail_class = np.isin(filtered_train_y, tail_class_indices)\\nis_head_class = np.isin(filtered_train_y, head_class_indices)\\n\\n# Separate the data into head and tail classes\\ntail_class_data_X = filtered_train_X[is_tail_class]\\ntail_class_data_y = filtered_train_y[is_tail_class]\\n\\nhead_class_data_X = filtered_train_X[is_head_class]\\nhead_class_data_y = filtered_train_y[is_head_class]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "cate_seen_train_y = to_categorical(seen_train_y, len(seen))#make train y to categorial/one hot vectors"
      ],
      "metadata": {
        "id": "nKF_eYbGYbrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cate_seen_train_y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPD-lDL4ToFy",
        "outputId": "c2e72a13-b22f-4328-fb54-59b201fd619e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seen_train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx8XIrmZVdvP",
        "outputId": "520a482a-dc9a-42ba-938d-193b4bd1b964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  2,  9, ...,  4, 18, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Network, in the paper, I use pretrained google news embedding, here I do not use it and set the embedding layer trainable\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers import Embedding, Input, Concatenate\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "\n",
        "def Network(MAX_SEQUENCE_LENGTH = 3000, EMBEDDING_DIM = 300, nb_word = len(word_to_idx)+2, filter_lengths = [3, 4, 5],\n",
        "    nb_filter = 150, hidden_dims =250):\n",
        "\n",
        "    graph_in = Input(shape=(MAX_SEQUENCE_LENGTH,  EMBEDDING_DIM))\n",
        "    convs = []\n",
        "    for fsz in filter_lengths:\n",
        "        conv = Conv1D(filters=nb_filter,\n",
        "                                 kernel_size=fsz,\n",
        "                                 padding='valid',\n",
        "                                 activation='relu')(graph_in)\n",
        "        pool = GlobalMaxPooling1D()(conv)\n",
        "        convs.append(pool)\n",
        "\n",
        "    if len(filter_lengths)>1:\n",
        "        out = Concatenate(axis=-1)(convs)\n",
        "    else:\n",
        "        out = convs[0]\n",
        "\n",
        "    graph = Model(inputs=graph_in, outputs=out) #convolution layers\n",
        "\n",
        "    emb_layer = [Embedding(nb_word,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True),\n",
        "                 Dropout(0.2)\n",
        "        ]\n",
        "    conv_layer = [\n",
        "            graph,\n",
        "        ]\n",
        "    feature_layers1 = [\n",
        "            Dense(hidden_dims),\n",
        "            Dropout(0.2),\n",
        "            Activation('relu')\n",
        "    ]\n",
        "    feature_layers2 = [\n",
        "            Dense(len(seen) ),\n",
        "            Dropout(0.2),\n",
        "    ]\n",
        "    output_layer = [\n",
        "            Activation('sigmoid')\n",
        "    ]\n",
        "\n",
        "    model = Sequential(emb_layer+conv_layer+feature_layers1+feature_layers2+output_layer)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Y6ZzMVuaVfg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network()\n",
        "print( model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtJ4zBCxVoVC",
        "outputId": "e403f074-3ee4-46c0-8641-083a28f4a483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3000, 300)         3672900   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3000, 300)         0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 450)               540450    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               112750    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 250)               0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 250)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                6024      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 24)                0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 24)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4332124 (16.53 MB)\n",
            "Trainable params: 4332124 (16.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestmodel_path = 'bestmodel.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=bestmodel_path, verbose=1, save_best_only=True)\n",
        "early_stopping=EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "model.fit(seen_train_X,\n",
        "          cate_seen_train_y,\n",
        "          epochs=100,\n",
        "          batch_size=128,\n",
        "          callbacks=[checkpointer, early_stopping],\n",
        "          validation_split=0.2,\n",
        "          class_weight=class_weight_dict)\n",
        "\n",
        "model.load_weights(bestmodel_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCDHjOsaeqKT",
        "outputId": "e4a88d14-6e72-4dbc-b8b6-5b16692d2c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.0708\n",
            "Epoch 1: val_loss improved from inf to 0.19368, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 55s 691ms/step - loss: 0.3181 - accuracy: 0.0708 - val_loss: 0.1937 - val_accuracy: 0.0997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.0856\n",
            "Epoch 2: val_loss improved from 0.19368 to 0.15313, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 45s 704ms/step - loss: 0.2703 - accuracy: 0.0856 - val_loss: 0.1531 - val_accuracy: 0.4455\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.3098\n",
            "Epoch 3: val_loss improved from 0.15313 to 0.09749, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 39s 608ms/step - loss: 0.2244 - accuracy: 0.3098 - val_loss: 0.0975 - val_accuracy: 0.6172\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.4842\n",
            "Epoch 4: val_loss improved from 0.09749 to 0.07933, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 41s 640ms/step - loss: 0.1980 - accuracy: 0.4842 - val_loss: 0.0793 - val_accuracy: 0.7089\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.5709\n",
            "Epoch 5: val_loss improved from 0.07933 to 0.06670, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 38s 595ms/step - loss: 0.1826 - accuracy: 0.5709 - val_loss: 0.0667 - val_accuracy: 0.7425\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.6230\n",
            "Epoch 6: val_loss improved from 0.06670 to 0.06150, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 39s 608ms/step - loss: 0.1727 - accuracy: 0.6230 - val_loss: 0.0615 - val_accuracy: 0.7597\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.6770\n",
            "Epoch 7: val_loss improved from 0.06150 to 0.05740, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 37s 581ms/step - loss: 0.1670 - accuracy: 0.6770 - val_loss: 0.0574 - val_accuracy: 0.7750\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.7119\n",
            "Epoch 8: val_loss improved from 0.05740 to 0.05336, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 38s 591ms/step - loss: 0.1597 - accuracy: 0.7119 - val_loss: 0.0534 - val_accuracy: 0.7879\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.7490\n",
            "Epoch 9: val_loss improved from 0.05336 to 0.05057, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 37s 585ms/step - loss: 0.1536 - accuracy: 0.7490 - val_loss: 0.0506 - val_accuracy: 0.7893\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.7771\n",
            "Epoch 10: val_loss improved from 0.05057 to 0.04975, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 37s 587ms/step - loss: 0.1491 - accuracy: 0.7771 - val_loss: 0.0497 - val_accuracy: 0.7943\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.8045\n",
            "Epoch 11: val_loss improved from 0.04975 to 0.04942, saving model to bestmodel.h5\n",
            "64/64 [==============================] - 37s 582ms/step - loss: 0.1464 - accuracy: 0.8045 - val_loss: 0.0494 - val_accuracy: 0.7913\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.8189\n",
            "Epoch 12: val_loss did not improve from 0.04942\n",
            "64/64 [==============================] - 37s 576ms/step - loss: 0.1460 - accuracy: 0.8189 - val_loss: 0.0496 - val_accuracy: 0.7977\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.8272\n",
            "Epoch 13: val_loss did not improve from 0.04942\n",
            "64/64 [==============================] - 36s 566ms/step - loss: 0.1431 - accuracy: 0.8272 - val_loss: 0.0500 - val_accuracy: 0.7933\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.8428\n",
            "Epoch 14: val_loss did not improve from 0.04942\n",
            "64/64 [==============================] - 37s 576ms/step - loss: 0.1412 - accuracy: 0.8428 - val_loss: 0.0512 - val_accuracy: 0.7953\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.8472\n",
            "Epoch 15: val_loss did not improve from 0.04942\n",
            "64/64 [==============================] - 36s 553ms/step - loss: 0.1421 - accuracy: 0.8472 - val_loss: 0.0521 - val_accuracy: 0.7943\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.8518\n",
            "Epoch 16: val_loss did not improve from 0.04942\n",
            "64/64 [==============================] - 35s 543ms/step - loss: 0.1411 - accuracy: 0.8518 - val_loss: 0.0530 - val_accuracy: 0.7938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict on training examples for cauculate standard deviation\n",
        "seen_train_X_pred = model.predict(seen_train_X)\n",
        "print(seen_train_X_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhEhUXKSevUm",
        "outputId": "04420a13-3270-4606-811f-e99dea3e92a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317/317 [==============================] - 12s 33ms/step\n",
            "(10133, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit a gaussian model\n",
        "from scipy.stats import norm as dist_model\n",
        "def fit(prob_pos_X):\n",
        "    prob_pos = [p for p in prob_pos_X]+[2-p for p in prob_pos_X]\n",
        "    pos_mu, pos_std = dist_model.fit(prob_pos)\n",
        "    return pos_mu, pos_std"
      ],
      "metadata": {
        "id": "NqaSwLLZe0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seen_train_X_pred[seen_train_y==1, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh2HGbKee6_m",
        "outputId": "f1c9a336-1b17-4b78-fd0d-c363cfcfd835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.85150397e-01, 9.79143023e-01, 9.92367804e-01, 9.77058351e-01,\n",
              "       9.91540611e-01, 9.81802940e-01, 9.85028148e-01, 9.73239362e-01,\n",
              "       9.93817747e-01, 9.95275736e-01, 9.96174335e-01, 3.73902231e-01,\n",
              "       9.94474351e-01, 9.97966409e-01, 8.94937217e-01, 9.64870155e-01,\n",
              "       9.74985123e-01, 9.87536609e-01, 9.64998901e-01, 9.87685561e-01,\n",
              "       9.66607988e-01, 9.89602268e-01, 9.98973370e-01, 9.97129261e-01,\n",
              "       9.54796135e-01, 9.86570776e-01, 9.82808411e-01, 9.92377400e-01,\n",
              "       9.36142266e-01, 9.52724755e-01, 9.90425050e-01, 9.98744130e-01,\n",
              "       9.84116375e-01, 9.98389244e-01, 9.88321960e-01, 9.94531095e-01,\n",
              "       9.91833210e-01, 9.93691802e-01, 8.84979725e-01, 9.93205905e-01,\n",
              "       9.96724308e-01, 9.83204663e-01, 9.83705997e-01, 9.98580575e-01,\n",
              "       9.76536751e-01, 9.95748222e-01, 6.60570443e-01, 9.96714115e-01,\n",
              "       8.01247239e-01, 9.92664278e-01, 9.42758203e-01, 9.95913088e-01,\n",
              "       9.14614797e-01, 9.94919360e-01, 9.96536255e-01, 9.79188383e-01,\n",
              "       9.90562379e-01, 9.50101316e-01, 9.48274076e-01, 9.46178436e-01,\n",
              "       9.97153282e-01, 9.97165143e-01, 9.48484123e-01, 9.97886479e-01,\n",
              "       9.88247693e-01, 9.77003276e-01, 8.45394433e-01, 9.48949575e-01,\n",
              "       6.14709973e-01, 9.92762029e-01, 9.93556321e-01, 9.95518267e-01,\n",
              "       9.78447497e-01, 7.08541155e-01, 9.77471292e-01, 9.97362912e-01,\n",
              "       9.96358216e-01, 9.99267519e-01, 9.45645452e-01, 9.97663140e-01,\n",
              "       9.94895637e-01, 9.90428925e-01, 8.87697637e-01, 9.94133890e-01,\n",
              "       9.12692726e-01, 8.80430341e-01, 9.91137624e-01, 9.84824359e-01,\n",
              "       9.88588691e-01, 9.97461438e-01, 9.92391527e-01, 9.65636611e-01,\n",
              "       9.95146096e-01, 9.94915247e-01, 9.63941991e-01, 9.79275346e-01,\n",
              "       9.89077568e-01, 9.82164502e-01, 9.95726228e-01, 9.78427410e-01,\n",
              "       9.98890221e-01, 9.98493433e-01, 9.80126679e-01, 9.86732423e-01,\n",
              "       9.95902956e-01, 9.83413100e-01, 5.74115098e-01, 9.90672112e-01,\n",
              "       9.95642900e-01, 9.91070271e-01, 9.99151945e-01, 9.32801664e-01,\n",
              "       9.99039471e-01, 9.61841106e-01, 9.84073758e-01, 9.94284570e-01,\n",
              "       9.52259064e-01, 9.94838774e-01, 9.94636476e-01, 9.63891864e-01,\n",
              "       9.96776640e-01, 9.00018513e-01, 9.72169995e-01, 9.83491719e-01,\n",
              "       3.93692970e-01, 9.91769075e-01, 9.72067833e-01, 9.96929109e-01,\n",
              "       9.84600842e-01, 9.95872438e-01, 9.68615890e-01, 9.98693764e-01,\n",
              "       8.22232842e-01, 9.89334047e-01, 9.88455296e-01, 9.96131539e-01,\n",
              "       9.67717767e-01, 9.54835832e-01, 9.92247462e-01, 9.94719863e-01,\n",
              "       9.90955532e-01, 9.97084677e-01, 9.94471908e-01, 9.97621715e-01,\n",
              "       9.73808885e-01, 9.98804212e-01, 9.29939032e-01, 9.83066678e-01,\n",
              "       9.51732814e-01, 9.79940593e-01, 8.78646553e-01, 9.80344951e-01,\n",
              "       9.87102807e-01, 9.44565415e-01, 9.52158391e-01, 9.24065650e-01,\n",
              "       9.84607160e-01, 9.66760397e-01, 9.71093118e-01, 9.27911639e-01,\n",
              "       9.77934301e-01, 9.97247636e-01, 9.95460212e-01, 9.95571494e-01,\n",
              "       8.57929945e-01, 9.81617212e-01, 9.71753180e-01, 9.79438901e-01,\n",
              "       9.93854523e-01, 9.81637061e-01, 9.98102486e-01, 9.27364469e-01,\n",
              "       9.64106202e-01, 8.64999056e-01, 9.89951670e-01, 9.77517247e-01,\n",
              "       9.90503311e-01, 9.93980289e-01, 9.97771800e-01, 9.96593177e-01,\n",
              "       9.46217418e-01, 9.59902465e-01, 9.83827591e-01, 5.12419820e-01,\n",
              "       9.98838961e-01, 9.81785834e-01, 9.96627271e-01, 9.80103314e-01,\n",
              "       6.50113106e-01, 9.94316399e-01, 9.75175858e-01, 9.85137224e-01,\n",
              "       7.67859936e-01, 9.32234824e-01, 6.33362412e-01, 9.78836834e-01,\n",
              "       9.84257460e-01, 9.86673236e-01, 9.91670012e-01, 8.90626669e-01,\n",
              "       9.87663090e-01, 9.47903097e-01, 9.11058545e-01, 9.95368481e-01,\n",
              "       8.71887624e-01, 7.29002416e-01, 8.79034162e-01, 9.37791944e-01,\n",
              "       8.92428756e-01, 9.69618738e-01, 8.89639080e-01, 9.69604731e-01,\n",
              "       9.89931583e-01, 7.22187996e-01, 9.91122246e-01, 9.99166012e-01,\n",
              "       9.98343110e-01, 9.90112126e-01, 9.86840010e-01, 9.63991761e-01,\n",
              "       9.91873205e-01, 7.83145607e-01, 9.69181597e-01, 8.50324214e-01,\n",
              "       9.46461141e-01, 9.91910994e-01, 9.45175767e-01, 9.20129776e-01,\n",
              "       9.92443740e-01, 9.95568097e-01, 9.66307580e-01, 9.86893892e-01,\n",
              "       9.75011170e-01, 9.20081139e-01, 8.27841341e-01, 9.98945177e-01,\n",
              "       9.93714392e-01, 9.98479545e-01, 9.95479465e-01, 9.97083724e-01,\n",
              "       9.94198322e-01, 9.91178572e-01, 7.38319159e-01, 9.79730368e-01,\n",
              "       9.80308175e-01, 9.81204093e-01, 9.98110771e-01, 9.97209489e-01,\n",
              "       6.16844416e-01, 8.75080168e-01, 9.81623292e-01, 6.20667040e-01,\n",
              "       8.73807967e-01, 9.78267193e-01, 9.96668637e-01, 8.75143945e-01,\n",
              "       9.05454636e-01, 9.29648817e-01, 6.46017671e-01, 6.54431760e-01,\n",
              "       9.98680055e-01, 9.30340946e-01, 9.96821284e-01, 9.97596085e-01,\n",
              "       9.88881290e-01, 9.47700322e-01, 3.60689253e-01, 9.50774133e-01,\n",
              "       9.81358588e-01, 9.42489922e-01, 9.86534357e-01, 9.82045829e-01,\n",
              "       9.65164661e-01, 9.97831166e-01, 9.90016043e-01, 9.95011926e-01,\n",
              "       9.85605657e-01, 9.56211925e-01, 9.96708632e-01, 9.68621373e-01,\n",
              "       9.91232812e-01, 9.79149401e-01, 9.93735790e-01, 9.98975396e-01,\n",
              "       9.91645634e-01, 9.93234098e-01, 9.94957268e-01, 9.74209011e-01,\n",
              "       7.64263809e-01, 9.85034704e-01, 9.99125183e-01, 9.97997344e-01,\n",
              "       9.83244777e-01, 9.88970041e-01, 9.97562647e-01, 9.82009947e-01,\n",
              "       9.38012481e-01, 9.96778905e-01, 9.08055663e-01, 9.92106915e-01,\n",
              "       9.82230961e-01, 9.96019304e-01, 7.78560281e-01, 9.96484518e-01,\n",
              "       9.97419238e-01, 9.21809971e-01, 9.83666062e-01, 9.87622321e-01,\n",
              "       9.94308293e-01, 8.89939308e-01, 9.96137202e-01, 9.96620536e-01,\n",
              "       9.82260406e-01, 9.98528004e-01, 9.29162085e-01, 6.47988021e-01,\n",
              "       9.95769620e-01, 9.76219118e-01, 9.89896297e-01, 9.39464271e-01,\n",
              "       9.93182242e-01, 9.91221666e-01, 9.89218533e-01, 9.92379308e-01,\n",
              "       9.71335769e-01, 9.73880589e-01, 9.95808363e-01, 9.94378746e-01,\n",
              "       9.72276270e-01, 9.86158788e-01, 9.87154186e-01, 7.38303959e-01,\n",
              "       9.89054918e-01, 9.94555116e-01, 9.72571850e-01, 9.95315194e-01,\n",
              "       9.99320149e-01, 9.83158112e-01, 9.38737094e-01, 9.30178583e-01,\n",
              "       9.49279130e-01, 9.63878632e-01, 9.96139944e-01, 9.13692892e-01,\n",
              "       9.96159673e-01, 2.88712054e-01, 9.87235606e-01, 9.53875184e-01,\n",
              "       8.64718318e-01, 9.95331824e-01, 9.97081697e-01, 9.64361012e-01,\n",
              "       9.78635848e-01, 1.05603203e-01, 9.16745663e-01, 4.84488755e-01,\n",
              "       9.94385600e-01, 9.86890435e-01, 9.26628470e-01, 9.92542624e-01,\n",
              "       9.34104621e-01, 9.34663117e-01, 9.94000614e-01, 9.96344388e-01,\n",
              "       9.96199787e-01, 9.87300098e-01, 9.96806741e-01, 9.28662241e-01,\n",
              "       3.19097221e-01, 9.87748027e-01, 9.91942883e-01, 9.92443264e-01,\n",
              "       9.83570933e-01, 9.63347793e-01, 9.08694625e-01, 9.85146582e-01,\n",
              "       9.89368856e-01, 9.83906269e-01, 9.96436954e-01, 9.54070568e-01,\n",
              "       8.56840134e-01, 1.95527598e-02, 9.93844151e-01, 9.88704979e-01,\n",
              "       9.91256833e-01, 9.93374288e-01, 9.95574713e-01, 9.79662359e-01,\n",
              "       9.87481594e-01, 8.47811460e-01, 7.80057847e-01, 9.95664775e-01,\n",
              "       9.93689179e-01, 9.97237682e-01, 9.94504631e-01, 9.80184376e-01,\n",
              "       7.97541022e-01, 9.88095760e-01, 9.85925615e-01, 9.94615734e-01,\n",
              "       9.97205794e-01, 9.86902475e-01, 7.52020895e-01, 9.96213257e-01,\n",
              "       9.72643852e-01, 9.94176507e-01, 9.94292855e-01, 8.74322534e-01,\n",
              "       9.85453486e-01, 3.85802835e-01, 9.96336937e-01, 9.99617219e-01,\n",
              "       9.81899500e-01, 9.97791886e-01, 9.84704852e-01, 9.14104730e-02,\n",
              "       9.65710342e-01, 9.98736799e-01, 9.98723209e-01, 9.83011603e-01,\n",
              "       9.81199622e-01, 9.96546805e-01, 8.39294016e-01, 9.93197560e-01,\n",
              "       9.21977222e-01, 9.66080487e-01, 9.94767547e-01, 9.98962522e-01,\n",
              "       9.74527061e-01, 9.82882917e-01, 9.63862598e-01, 9.42708194e-01,\n",
              "       9.90414619e-01, 9.92253006e-01, 9.87908781e-01, 6.03293538e-01,\n",
              "       9.64960158e-01, 9.96067226e-01, 9.86516833e-01, 9.77091849e-01,\n",
              "       8.47176433e-01, 9.89276648e-01, 9.84293163e-01, 9.96070147e-01,\n",
              "       9.96139586e-01, 9.92802799e-01, 9.85405445e-01, 9.98513877e-01,\n",
              "       8.83349895e-01, 9.92582440e-01, 9.72865403e-01, 6.98081672e-01,\n",
              "       9.93386924e-01, 9.84071136e-01, 9.84998643e-01, 9.98553216e-01,\n",
              "       9.94287431e-01, 9.88762200e-01, 9.97102678e-01, 6.59443915e-01,\n",
              "       9.91696835e-01, 9.94920433e-01, 9.88270760e-01, 9.90400493e-01,\n",
              "       9.95795131e-01, 9.73035395e-01, 9.83503699e-01, 9.96887386e-01,\n",
              "       9.94932592e-01, 9.57319498e-01, 9.91332471e-01, 9.92154181e-01,\n",
              "       9.81119633e-01, 9.93332565e-01, 9.11694109e-01, 9.95914996e-01,\n",
              "       9.74009514e-01, 9.85190928e-01, 9.92682517e-01, 9.78973448e-01,\n",
              "       5.66396475e-01, 8.47492754e-01, 9.95139122e-01, 9.94912744e-01,\n",
              "       7.91317821e-01, 9.76051569e-01, 9.98420954e-01, 9.70520675e-01,\n",
              "       9.84560609e-01, 9.71464217e-01, 9.91935611e-01, 9.99145031e-01,\n",
              "       9.96279061e-01, 9.90115285e-01, 9.34269965e-01, 2.58687377e-01,\n",
              "       9.88116503e-01, 9.97098804e-01, 9.72477317e-01, 9.51727629e-01,\n",
              "       9.88696933e-01, 9.94640052e-01, 9.97734547e-01, 9.25905228e-01,\n",
              "       9.23001349e-01, 9.40407217e-01, 9.75313842e-01, 9.82891917e-01,\n",
              "       9.98834789e-01, 9.98056829e-01, 8.87338400e-01, 9.94296134e-01,\n",
              "       9.86732423e-01, 9.97526348e-01, 9.95647848e-01, 9.84359205e-01,\n",
              "       9.95199919e-01, 9.88413513e-01, 9.88922834e-01, 9.87373650e-01,\n",
              "       9.99074101e-01, 9.83220220e-01, 9.89517748e-01, 9.29570735e-01,\n",
              "       9.90033209e-01, 9.99018192e-01, 9.84421372e-01, 9.84082639e-01,\n",
              "       9.71038580e-01, 8.78698707e-01, 9.93480921e-01, 5.11959434e-01,\n",
              "       8.53953898e-01, 9.84130025e-01, 9.31032062e-01, 9.89987433e-01,\n",
              "       9.57368672e-01, 9.95445251e-01, 9.97386634e-01, 9.68393743e-01,\n",
              "       9.94850576e-01, 9.46565807e-01, 9.96556044e-01, 9.92677093e-01,\n",
              "       7.00713769e-02, 9.89394724e-01, 9.94304001e-01, 9.81648088e-01,\n",
              "       9.83765781e-01, 9.99163985e-01, 9.88682568e-01, 9.95593011e-01,\n",
              "       9.78232384e-01, 9.48610246e-01, 9.11379635e-01, 9.98693407e-01,\n",
              "       9.91065562e-01, 9.96209979e-01, 3.69058788e-01, 9.17010128e-01,\n",
              "       9.98465180e-01, 8.00507665e-01, 9.99065220e-01, 9.84602571e-01,\n",
              "       9.97021616e-01, 9.63337600e-01, 9.71239269e-01, 9.90228295e-01,\n",
              "       1.12867318e-01, 9.56690073e-01, 9.96511042e-01, 8.56996924e-02,\n",
              "       9.78924215e-01, 9.94234979e-01, 7.80007243e-01, 9.52086031e-01,\n",
              "       9.86693799e-01, 9.60142910e-02, 9.69293237e-01, 9.40862417e-01,\n",
              "       9.84016180e-01, 9.56929326e-01, 9.95905519e-01, 7.95455396e-01,\n",
              "       9.84618843e-01, 1.07582507e-03, 9.85283732e-01, 9.89838362e-01,\n",
              "       9.96267736e-01, 9.73310828e-01, 9.88956034e-01, 1.20476699e-02,\n",
              "       9.96652663e-01, 9.91983831e-01, 9.95553434e-01, 9.91420150e-01,\n",
              "       9.97599900e-01, 3.37193571e-02, 4.01842408e-02, 6.00579202e-01,\n",
              "       2.81847026e-02, 1.09586395e-01, 9.72329319e-01, 9.96638775e-01,\n",
              "       9.86447096e-01, 9.88253295e-01, 9.88843203e-01, 8.89014184e-01,\n",
              "       9.70552862e-01, 6.67170525e-01, 9.93339360e-01, 9.83600438e-01,\n",
              "       9.61955428e-01, 9.92959797e-01, 9.93238807e-01, 9.58936214e-01,\n",
              "       9.97239113e-01, 7.30335414e-01, 1.60421982e-01, 9.84004617e-01,\n",
              "       9.66580033e-01, 9.96501565e-01, 9.61449556e-03, 7.66828835e-01,\n",
              "       5.85349184e-03, 9.95456934e-01, 9.88165975e-01, 9.34742689e-01,\n",
              "       8.16547096e-01, 9.79571879e-01, 9.69939828e-01, 9.03987288e-01,\n",
              "       9.14542496e-01, 9.87962425e-01, 9.98873889e-01, 7.22769350e-02,\n",
              "       8.55432868e-01, 9.78888333e-01, 9.93728995e-01, 9.89606321e-01,\n",
              "       9.83573258e-01, 9.68915939e-01, 9.96407330e-01, 1.50936395e-01,\n",
              "       9.32363212e-01, 9.65968430e-01, 2.31518596e-01, 9.97520983e-01,\n",
              "       3.04022562e-02, 9.72692728e-01, 9.68823195e-01, 9.62193072e-01,\n",
              "       9.32003379e-01, 9.81417358e-01, 9.99207199e-01, 3.09072156e-02,\n",
              "       9.48641360e-01, 9.60894167e-01, 4.31108288e-02, 9.85855281e-01,\n",
              "       1.25650138e-01, 3.04222256e-02, 5.93813062e-01, 1.79195493e-01,\n",
              "       9.80468631e-01, 9.91941333e-01, 7.41456344e-04, 9.80188608e-01,\n",
              "       8.47478926e-01, 9.92022872e-01, 9.76097703e-01, 9.79753792e-01,\n",
              "       8.22468281e-01, 9.19908285e-01, 9.90326464e-01, 8.87821019e-01,\n",
              "       9.88116741e-01, 9.91928697e-01, 9.79178071e-01, 9.99265850e-01,\n",
              "       9.50468719e-01, 8.61336946e-01, 9.95526850e-01, 4.34956141e-02,\n",
              "       9.74591613e-01, 9.50727403e-01, 9.87156510e-01, 9.92307007e-01,\n",
              "       9.63591099e-01, 9.61715996e-01, 9.91907477e-01, 9.87300456e-01,\n",
              "       4.44560796e-02, 9.89352167e-01, 5.84807396e-01, 6.43677652e-01,\n",
              "       9.93950486e-01, 9.96837020e-01, 9.86724734e-01, 6.81673467e-01,\n",
              "       9.99257982e-01, 1.43785968e-01, 2.49247968e-01, 9.94115829e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate mu, std of each seen class\n",
        "mu_stds = []\n",
        "for i in range(len(seen)):\n",
        "    pos_mu, pos_std = fit(seen_train_X_pred[seen_train_y==i, i])\n",
        "    mu_stds.append([pos_mu, pos_std])\n",
        "\n",
        "print( mu_stds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYegeINUfryw",
        "outputId": "0cccf596-7e71-490b-859a-baffe3f54ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 0.2532659776836448], [1.0, 0.22239745840716654], [1.0, 0.23068763398435194], [1.0, 0.31393179295583], [1.0, 0.19436211060253694], [1.0, 0.18633537483072082], [1.0, 0.16067463824920267], [1.0, 0.3184331370178253], [1.0, 0.3640887402312565], [1.0, 0.2955334273706855], [1.0, 0.31410519932115283], [1.0, 0.2037727962300163], [1.0, 0.3051365852606025], [1.0, 0.29348229847287954], [1.0, 0.2469284436779268], [1.0, 0.23593682926375592], [1.0, 0.2567065990963774], [1.0, 0.02568502759134975], [1.0, 0.21707438028292722], [1.0, 0.21083414626540395], [1.0, 0.1592079363892272], [1.0, 0.15122369784087691], [1.0, 0.14426584373029716], [1.0, 0.13806186364563763]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict on test examples\n",
        "test_X_pred = model.predict(np.concatenate([seen_test_X,unseen_test_X], axis = 0))\n",
        "test_y_gt = np.concatenate([seen_test_y,unseen_test_y], axis = 0)\n",
        "print( test_X_pred.shape, test_y_gt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uwf7Pf-fv2X",
        "outputId": "2e3caf94-9256-4116-85e1-c7e0cdfb29e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 16s 34ms/step\n",
            "(15000, 24) (15000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_pred = []\n",
        "scale = 1.\n",
        "for p in test_X_pred:# loop every test prediction\n",
        "    max_class = np.argmax(p)# predicted class\n",
        "    max_value = np.max(p)# predicted probability\n",
        "    threshold = max(0.5, 1. - scale * mu_stds[max_class][1])#find threshold for the predicted class\n",
        "    if max_value > threshold:\n",
        "        test_y_pred.append(max_class)#predicted probability is greater than threshold, accept\n",
        "    else:\n",
        "        test_y_pred.append(len(seen))#otherwise, reject\n"
      ],
      "metadata": {
        "id": "Ji1glh0pfzwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(test_y_gt, test_y_pred)\n",
        "print( 'macro fscore: ', np.mean(fscore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXFKJxBTf3P7",
        "outputId": "dfa1f255-9de0-4727-9da1-16b6666dae8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro fscore:  0.6476397665355047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(test_y_gt, test_y_pred)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8IRcderf5Yv",
        "outputId": "f1c49488-99e5-4cb0-f9fb-f9a02d182df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.74      0.54       271\n",
            "           1       0.95      0.86      0.90       296\n",
            "           2       0.72      0.77      0.74       294\n",
            "           3       0.59      0.53      0.56       306\n",
            "           4       0.54      0.64      0.58       295\n",
            "           5       0.71      0.70      0.70       276\n",
            "           6       0.77      0.72      0.75       281\n",
            "           7       0.26      0.55      0.35       333\n",
            "           8       0.19      0.65      0.30       292\n",
            "           9       0.68      0.60      0.64       310\n",
            "          10       0.71      0.61      0.66       305\n",
            "          11       0.86      0.79      0.82       291\n",
            "          12       0.67      0.29      0.41       294\n",
            "          13       0.76      0.42      0.54       299\n",
            "          14       0.78      0.53      0.63       293\n",
            "          15       0.66      0.45      0.53       295\n",
            "          16       0.57      0.41      0.48       318\n",
            "          17       0.99      0.91      0.94       310\n",
            "          18       0.78      0.52      0.62       277\n",
            "          19       0.76      0.68      0.72       296\n",
            "          20       0.46      0.77      0.58       309\n",
            "          21       1.00      0.69      0.82       284\n",
            "          22       0.95      0.81      0.87       302\n",
            "          23       0.89      0.75      0.82       307\n",
            "          24       0.71      0.67      0.69      7866\n",
            "\n",
            "    accuracy                           0.66     15000\n",
            "   macro avg       0.70      0.64      0.65     15000\n",
            "weighted avg       0.70      0.66      0.67     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrYk1jRogPPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}